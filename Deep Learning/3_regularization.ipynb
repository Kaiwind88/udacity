{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.465668\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 15.7%\n",
      "Minibatch loss at step 500: 2.396097\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.849980\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1500: 0.861933\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2000: 0.826364\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.855999\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.745121\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkjpnwJAkSZ37P3G3GPrXuXpyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15e4ebed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 664.905640\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 30.0%\n",
      "Minibatch loss at step 500: 199.019821\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1000: 114.670189\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500: 68.522049\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2000: 41.299084\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.228905\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 15.522506\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmRXOBYaZmZkVzgWGmZmZFe7/8QLuMpJh0RcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16118ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 335.567749\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 31.8%\n",
      "Minibatch loss at step 2: 1255.971680\n",
      "Minibatch accuracy: 38.3%\n",
      "Validation accuracy: 39.0%\n",
      "Minibatch loss at step 4: 313.029846\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 59.9%\n",
      "Minibatch loss at step 6: 70.417320\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 8: 7.110050\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 10: 5.920082\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.4%\n",
      "Test accuracy: 71.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 429.683533\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 2: 1174.402832\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 34.7%\n",
      "Minibatch loss at step 4: 236.072418\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 52.7%\n",
      "Minibatch loss at step 6: 19.795319\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 8: 3.067132\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 10: 36.587135\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 12: 2.067960\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 14: 4.679473\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 63.3%\n",
      "Minibatch loss at step 16: 7.609349\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 18: 7.141596\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 20: 5.486036\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 24: 0.675438\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 26: 2.983878\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 30: 6.613388\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 32: 0.281949\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 34: 0.183335\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 38: 6.709373\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 44: 6.578982\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 46: 1.866381\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 50: 0.447444\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 52: 1.077528\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 56: 1.743884\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 60: 0.278701\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 64: 0.944413\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 66: 1.090819\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 68: 0.081017\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 70: 2.641829\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 74: 0.844981\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 84: 4.909534\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 86: 5.746615\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 90: 0.311856\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 92: 0.000083\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 94: 0.237545\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Test accuracy: 73.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.310769\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 27.5%\n",
      "Minibatch loss at step 500: 0.922657\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1000: 0.856786\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500: 0.565315\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.504377\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500: 0.548999\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3000: 0.548915\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3500: 0.573309\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.446269\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4500: 0.446617\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 5000: 0.519856\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5500: 0.479102\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.550986\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 6500: 0.393277\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.520245\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7500: 0.497338\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.571850\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8500: 0.418215\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 9000: 0.470590\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.368422\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 500: 0.336324\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1000: 0.438610\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.250834\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 2000: 0.238924\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.282517\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.350023\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3500: 0.324795\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 0.269526\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4500: 0.222062\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.313966\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5500: 0.225214\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6000: 0.321295\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6500: 0.183675\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.276892\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7500: 0.266043\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.292052\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8500: 0.135701\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.179820\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9500: 0.209468\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 10000: 0.168820\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10500: 0.145742\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11000: 0.116329\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11500: 0.150373\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 12000: 0.166362\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 12500: 0.120881\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 13000: 0.175748\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13500: 0.076927\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 14000: 0.155162\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 14500: 0.100123\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15000: 0.088235\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 15500: 0.089404\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 16000: 0.049796\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 16500: 0.059458\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17000: 0.021326\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17500: 0.009470\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 18000: 0.044952\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.1%\n",
      "Test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge! That's my best score on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
